# ğŸ“š Book RAG Assistant

A production-ready, offline Retrieval-Augmented Generation (RAG) application for analyzing PDF books using local AI. Built with Streamlit, LangChain, Ollama, and ChromaDB.

## âœ¨ Features

- **ğŸ”’ 100% Offline**: All processing happens locally on your machine
- **ğŸ“– Multi-Book Support**: Upload and manage multiple PDF books
- **ğŸ’¬ Chat Interface**: Ask questions about your books with context-aware answers
- **ğŸ“ Smart Summaries**: Generate comprehensive book summaries
- **ğŸ“Š Analytics**: Track usage statistics and book insights
- **ğŸ’¾ Persistent Storage**: Save uploaded books, chat history, and summaries
- **ğŸ³ Docker Support**: Easy deployment with Docker and docker-compose
- **ğŸ“± Modern UI**: Beautiful, responsive interface with Streamlit

## ğŸ—ï¸ Architecture

```
Book RAG Assistant
â”œâ”€â”€ ğŸ“„ PDF Upload & Processing
â”œâ”€â”€ ğŸ” Text Extraction (PyMuPDF + pdfminer)
â”œâ”€â”€ ğŸ“ Text Chunking & Embedding
â”œâ”€â”€ ğŸ—„ï¸ Vector Storage (ChromaDB)
â”œâ”€â”€ ğŸ¤– Local LLM (Ollama + llama2)
â”œâ”€â”€ ğŸ’¬ Q&A Chain (LangChain)
â”œâ”€â”€ ğŸ“Š Database (SQLite)
â””â”€â”€ ğŸ¨ Web UI (Streamlit)
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- 8GB+ RAM (for LLM processing)
- 5GB+ free disk space

### Option 1: Local Installation

1. **Clone the repository**

   ```bash
   git clone <repository-url>
   cd book-rag-assistant
   ```

2. **Run the setup script**

   ```bash
   python setup_production.py
   ```

3. **Start the application**

   ```bash
   streamlit run app.py
   ```

4. **Access the app**
   Open http://localhost:8501 in your browser

### Option 2: Docker Deployment

1. **Build and run with Docker Compose**

   ```bash
   docker-compose up --build
   ```

2. **Access the app**
   Open http://localhost:8501 in your browser

## ğŸ“‹ Installation Details

### Manual Setup

1. **Install Python dependencies**

   ```bash
   pip install -r requirements.txt
   ```

2. **Install Ollama**

   ```bash
   curl -fsSL https://ollama.ai/install.sh | sh
   ```

3. **Start Ollama and pull the model**

   ```bash
   ollama serve
   ollama pull llama2
   ```

4. **Run the application**
   ```bash
   streamlit run app.py
   ```

## ğŸ¯ Usage Guide

### 1. Upload Books

- Click "Choose PDF files" in the sidebar
- Select one or more PDF files
- Click "Process Uploaded Files"
- Wait for processing to complete

### 2. Chat with Books

- Select a book from the sidebar
- Go to the "ğŸ’¬ Chat" tab
- Ask questions about the book content
- View source citations from the original text

### 3. Generate Summaries

- Select a book from the sidebar
- Go to the "ğŸ“ Summary" tab
- Click "Generate Summary"
- Download the summary as a text file

### 4. View Analytics

- Go to the "ğŸ“Š Analytics" tab
- View statistics about your books
- Track usage and activity

## ğŸ› ï¸ Configuration

The application uses a `config.ini` file for configuration:

```ini
[APP]
debug = false
log_level = INFO

[DATABASE]
path = books.db

[STORAGE]
uploads_dir = uploads
chroma_dir = chroma_db

[OLLAMA]
model = llama2
host = http://localhost:11434
```

## ğŸ“ Project Structure

```
book-rag-assistant/
â”œâ”€â”€ app.py                 # Main application
â”œâ”€â”€ ui.py                  # UI components
â”œâ”€â”€ database.py            # Database management
â”œâ”€â”€ pdf_utils.py           # PDF processing
â”œâ”€â”€ rag_chain.py           # RAG chains
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile            # Docker configuration
â”œâ”€â”€ docker-compose.yml    # Docker Compose setup
â”œâ”€â”€ setup_production.py   # Production setup script
â”œâ”€â”€ uploads/              # Uploaded PDF files
â”œâ”€â”€ chroma_db/            # Vector database
â””â”€â”€ books.db              # SQLite database
```

## ğŸ”§ Development

### Running Tests

```bash
python test_pdf.py
python test_setup.py
```

### Adding New Features

1. Create feature branch
2. Implement changes
3. Update tests
4. Submit pull request

## ğŸ³ Docker Deployment

### Production Deployment

```bash
# Build and run
docker-compose up --build -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Custom Configuration

```bash
# Override environment variables
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up
```

## ğŸ“Š Performance

### System Requirements

- **Minimum**: 4GB RAM, 2 CPU cores
- **Recommended**: 8GB RAM, 4 CPU cores
- **Storage**: 5GB+ for models and data

### Optimization Tips

- Use SSD storage for better I/O performance
- Increase Docker memory limits for large PDFs
- Monitor Ollama resource usage

## ğŸ”’ Security

- All data stays on your local machine
- No external API calls (except model downloads)
- SQLite database with local file storage
- No user authentication required (single-user app)

## ğŸ› Troubleshooting

### Common Issues

1. **Ollama not running**

   ```bash
   ollama serve
   ```

2. **Model not found**

   ```bash
   ollama pull llama2
   ```

3. **Port already in use**

   ```bash
   streamlit run app.py --server.port 8502
   ```

4. **Memory issues**
   - Reduce chunk size in `rag_chain.py`
   - Use smaller models (llama2:7b instead of llama2:13b)

### Logs

- Application logs: Check Streamlit output
- Ollama logs: `ollama logs`
- Docker logs: `docker-compose logs`

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

see LICENSE file for details

## ğŸ™ Acknowledgments

- [Streamlit](https://streamlit.io/) for the web framework
- [LangChain](https://langchain.com/) for the RAG framework
- [Ollama](https://ollama.ai/) for local LLM inference
- [ChromaDB](https://www.trychroma.com/) for vector storage
- [PyMuPDF](https://pymupdf.readthedocs.io/) for PDF processing

## ğŸ“ Support

For issues and questions:

- Create an issue on GitHub
- Check the troubleshooting section
- Review the logs for error details
